<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Troubleshooting on 妈妈说只要名字长就有笨比跟着念-服务端技术分享</title>
    <link>https://www.ileopold.cn/categories/troubleshooting/</link>
    <description>Recent content in Troubleshooting on 妈妈说只要名字长就有笨比跟着念-服务端技术分享</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 27 Feb 2023 17:02:32 +0800</lastBuildDate><atom:link href="https://www.ileopold.cn/categories/troubleshooting/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Golang PProf工具使用</title>
      <link>https://www.ileopold.cn/posts/golang-pprof/</link>
      <pubDate>Mon, 27 Feb 2023 17:02:32 +0800</pubDate>
      
      <guid>https://www.ileopold.cn/posts/golang-pprof/</guid>
      <description>背景 工作中经常会遇到一些常见的Go问题，比如锁竞争、内存占用过高、CPU 占用过高、协程泄露、阻塞操作、GC频繁内存回收等，我们可以通过官方的PProf工具进行迅速排查定位问题，最近工作的2家公司线上服务也都集成了PProf采集和分析的WEB UI工具，所以这是一个非常实在的功能。
使用 一般的基本引入包自动init，并且开启http服务就会默认注册debug路由，如下所示：
import	_ &amp;#34;net/http/pprof&amp;#34; //nolint // ... _ = http.ListenAndServe(&amp;#34;:8080&amp;#34;, nil) 常用的性能分析指标有：
   类型 描述     profile CPU占用情况   heap 堆上内存的使用情况   goroutine 当前所有协程的情况   allocs 内存分配情况   blocks 阻塞操作情况   trace 程序运行跟踪情况    对于在线采集分析可以用下面的命令：
// cpu go tool pprof http://localhost:8080/debug/pprof/profile // 堆内存 go tool pprof http://localhost:8080/debug/pprof/heap // 协程 go tool pprof http://localhost:8080/debug/pprof/goroutine // etc .</description>
    </item>
    
    <item>
      <title>Redis分布式限流器</title>
      <link>https://www.ileopold.cn/posts/redis-rate-limiter/</link>
      <pubDate>Wed, 22 Feb 2023 22:48:51 +0800</pubDate>
      
      <guid>https://www.ileopold.cn/posts/redis-rate-limiter/</guid>
      <description>压测场景下，需要准确控制endpoint的发压QPS，因为压测机器很多，所以需要一个分布式限流器来对压力进行控制。
策略上 基于令牌桶 参考go-zero中的tokenlimit实现，原理即通过eval执行下面的lua脚本，一句话概括就是对于一个key，每次请求获取最近一次变更的时间戳以及剩余的token数，计算时间差按照平均速率能增加多少token，总和token数能不能满足本次的token数需求：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  local rate = tonumber(ARGV[1]) local capacity = tonumber(ARGV[2]) local now = tonumber(ARGV[3]) -- current timestamp may need redis.call(&amp;#34;TIME&amp;#34;) local requested = tonumber(ARGV[4]) local fill_time = capacity/rate local ttl = math.floor(fill_time*2) local last_tokens = tonumber(redis.call(&amp;#34;get&amp;#34;, KEYS[1])) if last_tokens == nil then last_tokens = capacity end local last_refreshed = tonumber(redis.</description>
    </item>
    
    <item>
      <title>Go Runtime Throws Gorountine Stack Dumps</title>
      <link>https://www.ileopold.cn/posts/go-runtime-throws-goruntine-stack/</link>
      <pubDate>Mon, 20 Feb 2023 21:18:07 +0800</pubDate>
      
      <guid>https://www.ileopold.cn/posts/go-runtime-throws-goruntine-stack/</guid>
      <description>同事抛来一个错误日志，让我通过日志定位问题，日志关键内容如下：
STDOUT detail: application crashes/exits early: application exits with code 2 ....blahblah... STDOUT goroutine 2070 [IO wait]: ....blahblah... STDOUT goroutine 2098 [chan receive]: ....blahblah... STDOUT goroutine 1771 [chan receive]: ....blahblah... 以前也在线上环境看到过类似的日志，当时就是觉得和pprof采集的goroutine堆栈信息比较像，但是单看他发的这几个goroutine的堆栈代码，代码逻辑没有问题，于是开始怀疑他给的日志不全，主动上平台查看日志发现完整的日志文件被截断了，查看完整的文件发现打印了一堆协程的堆栈，其中第一行打印了具体的原因：
fatal error: concurrent map read and map write goroutine 391 [running]: runtime.throw({0x330ef38?, 0x2c82480?}) /usr/local/go/src/runtime/panic.go:992 +0x71 fp=0xc003371488 sp=0xc003371458 pc=0x436491 runtime.mapaccess2_faststr(0x0?, 0x0?, {0xc0026e8497, 0x28}) /usr/local/go/src/runtime/map_faststr.go:117 +0x3d4 fp=0xc0033714f0 sp=0xc003371488 pc=0x4133b4 git.blahblah.com/blahblah/internal/clients/rate_limit/distribute/v2.findLimiter({0xc0026e8497, 0x28}) 继而查看业务代码，确实是在并发操作map时，可能存在写map的同时，有协程对map进行读取，导致fatal error，关于这个问题，之前在 Golang Map 源码分析 中提到过。
那这些goroutine的日志是谁打印的呢，以上面的错误为例，查看源码，是runtime throw出来的：</description>
    </item>
    
  </channel>
</rss>
